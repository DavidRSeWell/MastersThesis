Games are a common activity of humans. They exist and have existed in every known society in various forms. The nice thing about games from a computer science or game theory perspective is that they provide a well defined set of actions and outcomes. This allows scientists to apply mathematical tools to these toy environments in order to test various hypothesis that would otherwise be intractible in more complex environments. The hope is of course that we can learn some sort of general principles from experimenting in these game setting that we can than apply in the real world. One of the interesting things about the history of AlphaGo is that there is a trend toward generality. That is that the algorithms themselves require less and less input from a human in order to be able to learn in more and more settings. In this paper I will refer to AlphaGo as the class of algorithms invented by deepmind and not to any one specific instance. 

In 2014 Deepmind a UK based artificial intelligence company began their AlphaGo project. The goal was to see how deep learning could be utilized to beat the the game of Go. This was a tall order and up to that point previous algorithms had 
only managed to play at an amateur level. In 2015 the first version of ALphaGo beats Fan Hui the European Go champion at the time. The first article describing their methodology was released in 2016 and they dubbed the name of this AlphaGo version AlphaGo Fan. AlphaGo Lee was the next version to be released. With only a few tweaks to AlphaGo Fan they were able to defeat the world champion Lee Sedol 4-1. The next version in 2017 AlphaGo Master beat top professionals 60-0. AlphaGo Zero was then released in late 2017 and was trained without any human or expert data. AlphaZero is published in 2018 in which some of the Go specific constraints are dropped. AlphaZero successfully dominates Chess, Go and Shogi (Japanese Chess) in that release. In 2019 a paper entitled "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" is released. This version is even more general than previous versions and manages to beat benchmarks in Atari as well. We will briefly discuss the relevance of MuZero but will not go into detail here. In 2020 Jean-Bastien Grill and colleagues published "Monte-Carlo tree search as regularized policy optimization". This paper has some interesting theoretic claims in regards to AlphaGo and to MCTS in general and we will briefly discuss this paper as well. 