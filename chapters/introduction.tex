Games are a common activity of humans. They exist and have existed in every known society in various forms. The nice thing about games from a computer science or game theory perspective is that they provide a well defined decision making environment. This allows scientists to apply mathematical tools to these toy environments in order to test various hypothesis that would otherwise be intractable in more complex environments. The hope is of course that we can learn some sort of general principles from experimenting in these game setting that we can than apply in the real world. One of the interesting things about the history of AlphaZero is that there is a trend toward simplicity and generalization. That is that the algorithms themselves require less and less input from a human in order to be able to learn in more and more settings. Earlier versions are also more complex in their implementations and so a focus on AlphaZero I think helps to see the core priciples at work. 

It is intersting to look at the history of AlphaZero a bit. In 2014 Deepmind a UK based artificial intelligence company began their AlphaGo project. The goal was to see how deep learning could be utilized to beat the the game of Go. This was a tall order and up to that point previous algorithms had only managed to play at an amateur level. In 2015 the first version of ALphaGo beats Fan Hui the European Go champion at the time. The first article describing their methodology was released in 2016 and they dubbed the name of this AlphaGo version AlphaGo Fan. AlphaGo Lee was the next version to be released. With only a few tweaks to AlphaGo Fan they were able to defeat the world champion Lee Sedol 4-1. The next version in 2017 AlphaGo Master beat top professionals 60-0. AlphaGo Zero was then released in late 2017 and was trained without any human or expert data. AlphaZero is published in 2018 in which some of the Go specific constraints are dropped. AlphaZero successfully dominates Chess, Go and Shogi (Japanese Chess) in that release. In 2019 a paper entitled "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" is released. This version is even more general than previous versions and manages to beat benchmarks in Atari as well. We will briefly discuss the relevance of MuZero but will not go into detail here. In 2020 Jean-Bastien Grill and colleagues published "Monte-Carlo tree search as regularized policy optimization". This paper has some interesting theoretic claims in regards to AlphaGo and to MCTS in general and we will briefly discuss this paper and its relevance. Some very interesting applications have also been produced as a result of this research. AlphaFold in 2018 and AlphaFold2 in 2020 produced state of the are prediction on the protein folding problem. This allows AlphaFold to predict the structure of a protein just from its sequence of amino acids. This particular breakthrough from Deepmind has some really exciting implications for Medicine, computational biology, and just for our understanding of biology in general. 